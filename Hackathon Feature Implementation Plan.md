This is an excellent way to structure the hackathon workflow. By completely decoupling the conversational experience from the deterministic logic, Garv and your other teammate can build and test their systems independently using mock data until it is time to integrate.

Here is the detailed implementation plan, broken down into phases and milestones for both tracks.

### ---

**Track 1: The Conversational & Extraction Layer**

**Assignee:** Garv

**Focus:** LLM orchestration, semantic routing, and structured JSON extraction.

#### **Phase 1: Foundation (Semantic Routing & Chatbot)**

The first goal is to ensure the agent understands what the user is trying to do before blindly searching the database.

| Milestone | Task | Description |
| :---- | :---- | :---- |
| **1.1** | **Semantic Router** | Implement a lightweight classification layer at the entry point. This decides if the user's prompt is a casual greeting/general query or a specific intent to search for policies. |
| **1.2** | **Fallback Conversation** | Route non-policy intents to a standard LangChain ConversationChain. Equip this chain with conversation memory so the bot can maintain a natural dialogue without triggering the heavy retrieval pipeline. |

#### **Phase 2: Information Extraction (RAG Setup & Missing Info)**

Once the user is securely routed into the policy flow, the system must extract data reliably.

| Milestone | Task | Description |
| :---- | :---- | :---- |
| **2.1** | **Pydantic Schemas** | Define the required variables for each scheme (e.g., age, income, state, aadhar\_status) using strict Pydantic data models. |
| **2.2** | **Structured Output** | Configure the LangChain pipeline to force the Groq LLM to populate the schema. Use JsonOutputParser to ensure the model responds strictly with a parseable JSON object representing the user's profile. |
| **2.3** | **Conversational Loop** | Write a dictionary evaluation script. If the parsed JSON returns a null value for a critical key, the script pauses the flow and triggers a secondary prompt asking the user to provide that specific missing detail. |

### ---

**Track 2: The Logic & Retrieval Layer**

**Assignee:** Teammate 2

**Focus:** Python backend logic, deterministic rules, and Vector DB management.

#### **Phase 1: Core Logic (Deterministic Engine)**

This phase strips decision-making power away from the LLM to prevent hallucinations regarding eligibility.

| Milestone | Task | Description |
| :---- | :---- | :---- |
| **1.1** | **Policy Data Structuring** | Convert the dummy data (NSP Scholarship, PM-KISAN, Startup India) into a highly structured, machine-readable format (like nested Python dictionaries or JSON files). |
| **1.2** | **Boolean Engine** | Write the pure Python if/elif/else constraints. This engine accepts a mock user\_profile JSON and evaluates its key-value pairs strictly against the hardcoded policy limits. |
| **1.3** | **Guardrail Integration** | Ensure the output of this Python engine is an absolute True/False boolean that acts as the final gatekeeper before the user is told they can apply. |

#### **Phase 2: Advanced Retrieval (Next Best Action)**

When the rule engine rejects a user, the system should intelligently pivot.

| Milestone | Task | Description |
| :---- | :---- | :---- |
| **2.1** | **Intercept Logic** | Write the backend trigger that catches the "False" (ineligible) flag generated by the boolean engine in Phase 1\. |
| **2.2** | **Dynamic Re-Querying** | Extract the user's limiting constraints (e.g., their specific income bracket) and automatically construct a new semantic search without requiring user input. |
| **2.3** | **Metadata Filtering** | Implement ChromaDB metadata filtering (using operators like $lte or $eq). This ensures the next batch of retrieved schemes from the vector database automatically aligns with the user's demographic constraints. |

